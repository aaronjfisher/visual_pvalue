Supplemental Code
========================================================

This report was created using the knitr package (http://yihui.name/knitr/) in R (http://www.r-project.org/)

```{r setup,  results='hide', message=FALSE}
#Initial Setup
#Note: Output from this code section has been masked for the sake of saving space.

library(plotrix) 
library(lme4)

opts_chunk$set(dev = 'pdf')

#The code book for the two files below is included in the readMe files of the github repository.
x_full<-read.csv('coursera_user_responses_tidy.csv',header=TRUE) #user responses to coursera questions.
load('data_for_1plots_coursera.RData')  #objects describing the library of plots shown to users.

logit<-function(x) log(x/(1-x))
invLogit<-function(x) {
  out<-exp(x)/(1+exp(x)) #maintains the ability of the function to accept vectors
  out[exp(x)==Inf]<-1
	return(out)
}

```

```{r process-csv, results='hide', message=FALSE}
#Output from this section has been masked

#Process x_full

#re-order the style factor's levels so the reference category is first
uStyle<-unique(c('n100ref',as.character(x_full$style)))
x_full$style<-factor(as.character(x_full$style),levels=uStyle)
x_full$datVer<- as.factor(as.numeric(x_full$datVer))
x_full$id<- as.factor(as.numeric(x_full$id))
x_full$attemptNumFactor<- as.factor(as.numeric(x_full$attemptNum))

#preview x_full
dim(x_full) #x_full includes everything, including missing data
head(x_full) #note: styleNum values 2 & 3 are actually the same question style
lapply(x_full[1,],class)
N<-sum(!is.na(x_full$guessSig))
n<-sum(!duplicated(x_full$id[!is.na(x_full$guessSig)]))
K<-length(uStyle)
N #total # responses
n #total # users
K #total # of question types

#Make a version without missing data
x<- x_full[!is.na(x_full$guessSig),]

#Get number of answers for each question type
nStyle<-c(table(x$style))[uStyle]
nStyle


uStyle
pretty_style_labels<-c('Reference', 'Smaller n', 'Larger n', 'Best Fit','Axis Scale',
		'Axis Label', 'Outlier', 'Lowess') #for use in plots


######### How many users finished their first attempts of the quiz
    ## this section is commented out, as it runs fairly slowly.
# userNames<-unique(x$id)
# num_of_first_try_by_user<-rep(NA,n)
# for(i in 1:n){
#   num_of_first_try_by_user[i]<- sum(x$id==userNames[i] & x$attemptNum==1)
# }
# mean(num_of_first_try_by_user==9) #=.944
#########
```

Models for Baseline Accuracy, and Effect of Plot Presentation Style
-----------------

```{r initialGLMs, cache=TRUE}
#Just look at first attempts of the survey
attemptNum1Data<-x[x$attemptNum==1,]


#Using 2 separate models, one for sensitivity, one for specificity (sense & spec)
sum(attemptNum1Data$trueSig) # number of responses used in sense model = 9063
sum(!attemptNum1Data$trueSig) # number of responses used in spec model = 9032

glmmSense = glmer(correct ~ 1 + (1|id) + style, data=attemptNum1Data[attemptNum1Data$trueSig,],
	family="binomial") #Sensitivity model
glmmSpec = glmer(correct ~ 1 + (1|id) + style, data=attemptNum1Data[!attemptNum1Data$trueSig,],
	family="binomial") #Specificity model

#Show basic output
print(glmmSense,correlation=FALSE)
print(glmmSpec,correlation=FALSE)


```

Show odds ratios, CIs for odds ratios, and variance explained by random intercepts.

_**Note: some of the odds ratios reported here slightly vary from those in our [October 16, 2014 PeerJ paper](https://peerj.com/articles/589/), at around the 3rd decimal place.**_ This is because we ran our initial analysis under an earlier version of `lme4`. Loading the `lme4.0` package instead of `lme4` will reproduce our original results. These results are also visible in earlier versions of this write-up, in the "add knitr code" commit on Apr 16, 2014, in history of this repository. The `lme4.0` package is a maintained version of the `lme4` package designed to aid in reproducing results from earlier versions of `lme4`.

https://github.com/lme4/lme4/

```{r results_from_baseline_models, cache=TRUE, results='asis'}
#Function to get odds ratios and CIs for odds ratios
getORCIs<-function(model){ 
  #logit(E(Y))=X*Beta; 
  #odds(E(Y))=exp(X*Beta)
  modelCoef<-fixef(model)
  modelSe<-sqrt(diag(vcov(model))) #std error
  fitted_OR<-exp(modelCoef) 
  li<- exp(modelCoef - qnorm(.975) * modelSe)
  ui<- exp(modelCoef + qnorm(.975) * modelSe)

  out<- signif(cbind(fitted_OR, li, ui),3)
  rownames(out)<- pretty_style_labels
  kable(out)
}

# 95% Confidence intervals = (li, ui)
getORCIs(glmmSense)
getORCIs(glmmSpec)

#Show variance explained by the random intercepts in each model
#Works specifically for binomial models
get_var_explained_by_rand_int<-function(model){
	G<-attr(VarCorr(model)$id,'stddev')^2
	return( G/(G+(pi^2)/3) )
}
# get_var_explained_by_rand_int(glmmInt)
get_var_explained_by_rand_int(glmmSense)
get_var_explained_by_rand_int(glmmSpec)

```


Recreate two examples of plots shown to users.

```{r example_ref_plots, fig.height=5.005, fig.width=10.22}


par(mar=c(3,3.5,3,0),oma=c(1,.5,0,1),mfcol=c(1,2))

#Make example plots from the reference category, one significant and one not
plotSigRefInd<-which(pres=='n100ref'&pvals<.05)[4]
plotNotSigRefInd<-which(pres=='n100ref'&pvals>=.05)[4]

plot(xes[plotSigRefInd,],yes[plotSigRefInd,],xlab='',ylab='')
mtext(paste0('Example: Truly Significant Plot\n(p=',round(pvals[plotSigRefInd],3),')'),
  3,line=.4,font=2,cex=1.2)
mtext('X',1,line=2)
mtext('Y',2,line=2.5)
mtext('A)',3,line=1.5,adj=0,font=2,cex=1.2)

plot(xes[plotNotSigRefInd,],yes[plotNotSigRefInd,],xlab='',ylab='')
mtext(paste0('Example: Non-significant Plot\n(p=',round(pvals[plotNotSigRefInd],3),')'),3,line=.4,
  font=2,cex=1.2)
mtext('X',1,line=2)
mtext('B)',3,line=1.5,adj=0,font=2,cex=1.2)

```

Get confidence intervals on accuracy scale, as opposed to odds ratio scale, and plot results.

```{r accuracy_CIs, fig.height=5.229, fig.width=9.5, results='asis'}
#Function for generating confidence intervals for the fitted accuracy rates for each plot style;
#This function is meant to be used separately, for both the sensitivity and specificity models
getCIs<-function(model=glmmSense, plotInd=1:K, ci_width_scalar=1.96, plotIt=TRUE,
	axisLab=TRUE, cex.axis=1, offset=0, ref_lwd=2, ...){ #y can also be specificity
  #diag(vcov(model))
	#fixef(model)
	
	coefNames<-rep(NA,K)
	coefNames[1]<-'(Intercept)'
	coefNames[2:K]<-levels(x$style)[-1]
	modelCoef<-fixef(model)
	names(modelCoef)<-coefNames

	ui<- #upper CI (on probability/accuracy scale)
	li<- # Lower CI
	center<-rep(NA,8)
	for(k in 1:K){
		#let a be the vector such that crossprod(a,modelCoef) = intercept + coefficient[k]
		#abbreviate this crossproduct as 'af'
		a<-rep(0,K)
		names(a)<-coefNames
		a['(Intercept)']<-1
		if(k>1) a[coefNames[k]]<-1
		var_af<- t(a) %*% vcov(model) %*% a
		se_af<-sqrt(as.numeric(var_af))
		center_logOdds <- t(a)%*%modelCoef
		ui[k]<- invLogit( center_logOdds + ci_width_scalar*se_af)
		li[k]<- invLogit( center_logOdds - ci_width_scalar*se_af)
		center[k]<- invLogit(center_logOdds)
	}	

	if(plotIt){
		plotCI(x=center[plotInd]*100,y=offset+(length(plotInd)):1,ui=ui[plotInd]*100,
			li=li[plotInd]*100,pch=19,cex=.5,yaxt='n',err='x',ylab='', ...)
		if(axisLab) axis(2, at=1:(length(plotInd)), labels=pretty_style_labels[plotInd][(length(plotInd)):1],
			cex.axis=cex.axis,las=2) #need to reorder labels so they go down, not up
		abline(v=center[1]*100,lty=2,lwd=ref_lwd)
	}
	
	out<- cbind(center,ui,li)
	rownames(out)<- pretty_style_labels
	return(kable(out))
}


#plot confidence intervals for fitted accuracy
par(oma=c(0,4,0,0))
plotInd4CIfig_pre<-c(1,order(fixef(glmmSense)[-1])+1) #order plots by sense coef, not including ref category
plotInd4CIfig<-plotInd4CIfig_pre[plotInd4CIfig_pre!=7] #dropping outlier from plot
getCIs(glmmSense,plotInd=plotInd4CIfig,col=c("darkblue"),main='',xlab='',xlim=c(0,100),
	cex.axis=1.1,lwd=2,offset=.125,ylim=c(.8,7.2))
mtext('% Accuracy', side=1, line=2,cex=1.1)
mtext('Accuracy of Significance Classifications', side=3, line=.4,cex=1.2,font=2)
getCIs(glmmSpec,plotInd=plotInd4CIfig,col=c("darkred"),axisLab=FALSE,main='',xlab='',
	xlim=c(0,100),lwd=2,add=TRUE,offset=-.125)
abline(v=seq(0,100,by=20),col='darkgray',lty=2,lwd=2)
legend('bottomleft',c('Sensitivity', 'Specificity'),col=c('darkblue','darkred'),pch=19,bg='white')

```




Models for Learning
---------------


```{r learningModels, cache=TRUE}
#select users who did the survey at least twice, but exclude questions they saw twice.
users_with_multiple_tries<- unique(x$id[x$attemptNum>1])
multi_try_data_ind<-x$id %in% users_with_multiple_tries 
multi_try_data<-x[multi_try_data_ind& x$firstTry,]

#Calculate the percent of second attempts were discarded because users saw a duplicate plot
1-sum(multi_try_data_ind& x$firstTry)/sum(multi_try_data_ind)


########  Of users with multiple attempts, how many compeleted their first & second attempts?
userNames2<-unique(x[multi_try_data_ind,'id'])
multi_users_1st_tries<-
multi_users_2nd_tries<-rep(NA,length(userNames2))
for(i in 1:length(userNames2)){
  multi_users_2nd_tries[i]<- sum(x[multi_try_data_ind,'id']==userNames2[i] &
  	x[multi_try_data_ind,'attemptNum']==2)
  multi_users_1st_tries[i]<- sum(x[multi_try_data_ind,'id']==userNames2[i] & 
  	x[multi_try_data_ind,'attemptNum']==1)
}
mean(multi_users_1st_tries==9) #=.92
mean(multi_users_2nd_tries==9) #=.99
########


cut=2 #cutoff point for max # of tries in our model (only look at first and second attempts)
multi_try_data_sense_leq_cut<- multi_try_data[multi_try_data$attemptNum<=cut & multi_try_data$trueSig,]
multi_try_data_spec_leq_cut <- multi_try_data[multi_try_data$attemptNum<=cut & !multi_try_data$trueSig,]

#Fit random intercept model
dim(multi_try_data_sense_leq_cut) #846 = # responses in model
glmmSenseLearn_rIntercept = glmer(correct~ 1 +  (1|id) + attemptNumFactor*style,
  data=multi_try_data_sense_leq_cut, family="binomial") # Fit model with interaction terms
print(glmmSenseLearn_rIntercept,correlation=FALSE)

dim(multi_try_data_spec_leq_cut) #859 = # responses in model
glmmSpecLearn_rIntercept = glmer(correct ~ 1 +  (1|id) + attemptNumFactor*style,
	data=multi_try_data_spec_leq_cut, family="binomial") #
print(glmmSpecLearn_rIntercept,correlation=FALSE) 




#Show variance explained by random intercepts
get_var_explained_by_rand_int(glmmSenseLearn_rIntercept)
get_var_explained_by_rand_int(glmmSpecLearn_rIntercept)

#Get CIs for learning models
# Get confidence intervals for the fitted accuracy rates for each combination of style and 
# attempt number in the learning model. 
# Also get confidence intervals for odds ratios for the learning effect in each category.
# To get interpretation of effect of attemptNum, we need to add attemptNum coeff to interaction terms. 
# Negative interaction just means that the learning effect is less strong than in the reference category.
# The function below returns a list of CI matrixes for each style, with rows for attemptNum.
getCIlearn<-function(model,ci_width_scalar=1.96,test_type='two_sided'){    
	
	modelCoef<-fixef(model)
	coefNames<-names(modelCoef)

	#find value of 'cut' (max attemptNum in model)
	cut=length(coefNames)/K #K = number of categories for each attemptNum

	#list output by style
	CImats<-list()

	for(k in 1:K){
		k_mat<-matrix(NA,cut,8)
		colnames(k_mat)<-c('centerProb','liProb','uiProb','centerOR','liOR',
			'uiOR','zstat','p_value') #we'll add this to CImats later
		#attemptNum is the row index of k_mat
		for(no in 1:cut){
      # let a be the vector such that a'modelCoef = intercept + coefficient k
			#abbreviate this t(a)%*%modelCoef as af
      
			a<-rep(0,times=length(coefNames))
			names(a)<-coefNames
			a['(Intercept)']<-1
			if(k>1) a[paste0('style',uStyle[k])] <-1
			if(no>1) a[paste0('attemptNumFactor',no)]<-1
			if(k>1 & no>1) a[paste0('attemptNumFactor',no,':style',uStyle[k])]	<-1

			var_af<- t(a) %*% vcov(model) %*% a
			se_af<-sqrt(as.numeric(var_af))

			center_logOdds <- crossprod(a,modelCoef)
			k_mat[no,'uiProb']<- invLogit( center_logOdds + ci_width_scalar*se_af)
			k_mat[no,'liProb']<- invLogit( center_logOdds - ci_width_scalar*se_af)
			k_mat[no,'centerProb']<- invLogit(center_logOdds)
    

			# pvalues need to be calculated the same way, but without the intercept term,
			# and without the baseline style term.
			# Get dist of "wf"=t(w)%*%coefficients, where w is a vector similar to "a", above.
			if(no==1) next #This not relevant if a attemptNum=1
      
			w<-rep(0,times=length(coefNames))
      names(w)<-coefNames
      w[paste0('attemptNumFactor',no)]<-1
			if(k>1) w[paste0('attemptNumFactor',no,':style',uStyle[k])]	<-1

			var_wf<-t(w) %*% vcov(model) %*% w
			se_wf<-sqrt(as.numeric(var_wf))
      
      k_mat[no,'centerOR']<- exp(crossprod(w,modelCoef))
      k_mat[no,'uiOR']<- exp( crossprod(w,modelCoef) + ci_width_scalar*se_wf)
  		k_mat[no,'liOR']<- exp( crossprod(w,modelCoef) - ci_width_scalar*se_wf)
      
			zstat<- t(w)%*%modelCoef / se_wf
			if(test_type=="two_sided")      p_value<- 2*(1-pnorm(abs(zstat)))
			if(test_type=="one_sided_up")   p_value<-   (1-pnorm(zstat))
			if(test_type=="one_sided_down") p_value<-      pnorm(zstat)
			k_mat[no,'zstat']<- zstat
			k_mat[no,'p_value']<- p_value

		}
		CImats[[ uStyle[k] ]]<-k_mat
	}
	return(CImats)
}

#Show confidence intervals for odds ratios regarding the learning effect, for each category.
getCIlearn(glmmSenseLearn_rIntercept) 
getCIlearn(glmmSpecLearn_rIntercept)

```

Display results for learning.

```{r learning_accuracy, fig.width=9.175,fig.height=6.3958}



#Plotting (CIs for accuracy)
plot_LearnCImats<-function(ciMat, plotStyle='n100ref', type='sense', data=multi_try_data,
		offset=0, cex.axis=1.1, add=FALSE, ...){
	cut<-dim(ciMat)[1]
	thickness<-c(2,2)
  
  
	plotCI(x=ciMat[,'centerProb'], y=1:cut+offset, ui=ciMat[,'uiProb'], li=ciMat[,'liProb'],
		err='x', pch=19, cex=.5, xaxt='n', yaxt='n', lwd=thickness, xlim=c(-.05,1.05),
		ylim=c(.7,cut+.3), add=add, ...)
	if(add==FALSE){
	#need to reorder labels so they go down, not up
    axis(2, at=(1:cut), labels=1:cut,cex.axis=cex.axis) 
  	xLabels<-seq(0,1,length=6)
  	#need to reorder labels so they go down, not up
  	axis(1, at=xLabels, labels=xLabels*100,cex.axis=cex.axis) 
  	abline(v=seq(0,1,by=.2),lty=2,lwd=2,col='darkgray')	
    mtext(text='Attempt\nNumber',side=2,line=2.25,cex=cex.axis)
  }
}



ciMat_learn_sense <- getCIlearn(glmmSenseLearn_rIntercept)
ciMat_learn_spec  <- getCIlearn(glmmSpecLearn_rIntercept)




par(mfrow=c(3,1),mar=c(2,7,3,1),oma=c(3,0,0,0))

#REFERENCE
plot_LearnCImats(ciMat_learn_sense[['n100ref']],col='darkblue',plotStyle='n100ref',
	type='sense',xlab='',ylab='',offset=.125)
plot_LearnCImats(ciMat_learn_spec[['n100ref']],main='',col='darkred',
	plotStyle='n100ref',type='spec',xlab='',add=TRUE,offset=-.125)
mtext(text='Reference',side=3,line=.4,cex=1.2,font=2)

legend('topleft',c('Sensitivity', 'Specificity'),col=c('darkblue','darkred'),
	pch=19,cex=1.4,bg='white')

#SMALL N
plot_LearnCImats(ciMat_learn_sense[['n35']],col='darkblue',plotStyle='n35',
	type='sense',ylab='',xlab='',offset=.125)
plot_LearnCImats(ciMat_learn_spec[['n35']],main='',col='darkred',
	plotStyle='n35',type='spec',xlab='',ylab='',offset=-.125,add=TRUE)
mtext(text='Smaller n',side=3,line=.4,cex=1.2,font=2)

#BEST FIT
plot_LearnCImats(ciMat_learn_sense[['bestFit']],col='darkblue',ylab='',
	xlab='',plotStyle='bestFit',type='sense',offset=.125)
plot_LearnCImats(ciMat_learn_spec[['bestFit']],main='',col='darkred',
	plotStyle='bestFit',type='spec',xlab='',offset=-.125,add=TRUE)
mtext(text='Best Fit',side=3,line=.4,cex=1.2,font=2)

mtext(text='% Accuracy',side=1,line=3,cex=1.1)



```




