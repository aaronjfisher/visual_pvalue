Supplemental Code
========================================================

This report was created using the knitr package (http://yihui.name/knitr/) in R (http://www.r-project.org/)

```{r setup,  results='hide', message=FALSE}
#Initial Setup
#Note: Output from this code section has been masked for the sake of saving space.

library(plotrix) 
library(lme4)

#Note to editor: Code book for these two files will be a part of the online supplement
x_full<-read.csv('tidyCourseraPval.csv',header=TRUE) #user responses to coursera questions.
load('data_for_1plots_coursera.RData')  #objects describing the library of plots shown to users.

logit<-function(x) log(x/(1-x))
invLogit<-function(x) {
  out<-exp(x)/(1+exp(x)) #maintains the ability of the function to accept vectors
  out[exp(x)==Inf]<-1
	return(out)
}

```

```{r process-csv, results='hide', message=FALSE}
#Output from this section has been masked)

#Process x_full

#re-order the style factor's levels so the reference category is first
uStyle<-unique(c('n100ref',as.character(x_full$style)))
x_full$style<-factor(as.character(x_full$style),levels=uStyle)
x_full$datVer<- as.factor(as.numeric(x_full$datVer))
x_full$id<- as.factor(as.numeric(x_full$id))
x_full$attemptNumFactor<- as.factor(as.numeric(x_full$attemptNum))

#preview x_full
dim(x_full) #x_full includes everything, including missing data
head(x_full) #note: styleNum values 2 & 3 are actually the same question style
lapply(x_full[1,],class)
N<-sum(!is.na(x_full$guessSig))
n<-sum(!duplicated(x_full$id[!is.na(x_full$guessSig)]))
K<-length(uStyle)
N #total # responses
n #total # users
K #total # of question types

#Make a version without missing data
x<- x_full[!is.na(x_full$guessSig),]

#Get number of answers for each question type
nStyle<-c(table(x$style))[uStyle]
nStyle


uStyle
pretty_style_labels<-c('Reference','Smaller n','Larger n', 'Best Fit','Axis Scale', 'Axis Label', 'Outlier','Lowess') #for use in plots

```

Models for Baseline Accuracy, and Effect of Plot Presentation Style
-----------------

```{r initialGLMs, cache=TRUE}
#Just look at first attempts of the survey
attemptNum1Data<-x[x$attemptNum==1,]


#Using 2 separate models, one for sensitivity, one for specificity (sense & spec)
sum(attemptNum1Data$trueSig) # number of responses used in sense model = 9063
sum(!attemptNum1Data$trueSig) # number of responses used in spec model = 9032

glmmSense = glmer(correct ~ 1 + (1|id) + style,data=attemptNum1Data[attemptNum1Data$trueSig,],family="binomial") #Sensitivity model
glmmSpec = glmer(correct ~ 1 + (1|id) + style,data=attemptNum1Data[!attemptNum1Data$trueSig,],family="binomial") #Specificity model

#Show basic output
print(glmmSense,correlation=FALSE)
print(glmmSpec,correlation=FALSE)

#Get odds ratios and CIs
getORCIs<-function(model){ 
  #logit(E(Y))=X*Beta; 
  #odds(E(Y))=exp(X*Beta)
  modelCoef<-fixef(model)
  modelSe<-sqrt(diag(vcov(model))) #std error
  fit<-exp(modelCoef) 
  li<- exp(modelCoef - qnorm(.975) * modelSe)
  ui<- exp(modelCoef + qnorm(.975) * modelSe)
  signif(cbind(fit,li,ui),3)
}
getORCIs(glmmSense)
getORCIs(glmmSpec)

#Show variance explained by the random intercepts in each model
#Works specifically for binomial models
get_var_explained_by_rand_int<-function(model){
	G<-attr(VarCorr(model)$id,'stddev')^2
	return( G/(G+(pi^2)/3) )
}
# get_var_explained_by_rand_int(glmmInt)
get_var_explained_by_rand_int(glmmSense)
get_var_explained_by_rand_int(glmmSpec)

```


Get confidence intervals and plot results
```{r figure1, fig.width=7, fig.height=6}

#Generate Figure 1

#Function for generating confidence intervals for the fitted accuracy rates for each plot style;
#This function is meant to be used separately, for both the sensitivity and specificity models
getCIs<-function(model=glmmSense,plotInd=1:K,ci_width_scalar=1.96,plotIt=TRUE,axisLab=TRUE,cex.axis=1, ...){ #y can also be specificity
  #diag(vcov(model))
	#fixef(model)
	
	coefNames<-rep(NA,K)
	coefNames[1]<-'(Intercept)'
	coefNames[2:K]<-levels(x$style)[-1]
	modelCoef<-fixef(model)
	names(modelCoef)<-coefNames

	ui<- #upper CI (on probability/accuracy scale)
	li<- # Lower CI
	center<-rep(NA,8)
	for(k in 1:K){
		#let a be the vector such that crossprod(a,modelCoef) = intercept + coefficient[k]
		#abbreviate this crossproduct as 'af'
		a<-rep(0,K)
		names(a)<-coefNames
		a['(Intercept)']<-1
		if(k>1) a[coefNames[k]]<-1
		var_af<- t(a) %*% vcov(model) %*% a
		se_af<-sqrt(as.numeric(var_af))
		center_logOdds <- t(a)%*%modelCoef
		ui[k]<- invLogit( center_logOdds + ci_width_scalar*se_af)
		li[k]<- invLogit( center_logOdds - ci_width_scalar*se_af)
		center[k]<- invLogit(center_logOdds)
	}	

	if(plotIt){
		plotCI(x=center[plotInd]*100,y=(length(plotInd)):1,ui=ui[plotInd]*100,li=li[plotInd]*100,pch=19,cex=.5,yaxt='n',err='x',ylab='', ...)
		if(axisLab) axis(2, at=1:(length(plotInd)), labels=pretty_style_labels[plotInd][(length(plotInd)):1],cex.axis=cex.axis,las=2) #need to reorder labels so they go down, not up
		abline(v=center[1]*100,lty=2,col='darkgray')
	}
	
	return(cbind(center,ui,li,modelCoef))
}

########
#Generate Figure 1

par(mar=c(5,2,1,1),oma=c(1,7,5,1))
layoutMat<-cbind(rep(c(1,3),c(4,3)),rep(c(2,4),c(4,3)))
layout(layoutMat)

#Make example plots from the reference category, one significant and one not
plotSigRefInd<-which(pres=='n100ref'&pvals<.05)[4]
plotNotSigRefInd<-which(pres=='n100ref'&pvals>=.05)[4]

plot(xes[plotSigRefInd,],yes[plotSigRefInd,],xlab='',ylab='')
mtext('Truly Significant Plots',3,line=3,font=2,cex=1.3)
mtext(paste0('Sample Plot (p=',round(pvals[plotSigRefInd],3),')'),3,line=.2,font=1)
mtext('X',1,line=2)
mtext('Y',2,line=2.5)

plot(xes[plotNotSigRefInd,],yes[plotNotSigRefInd,],xlab='',ylab='')
mtext('Non-significant Plots',3,line=3,font=2,cex=1.3)
mtext(paste0('Sample Plot (p=',round(pvals[plotNotSigRefInd],3),')'),3,line=.2,font=1)
mtext('X',1,line=2)

#plot confidence intervals for fitted accuracy
plotInd4CIfig_pre<-order(fixef(glmmSense)) #order plots by sense coef
plotInd4CIfig<-plotInd4CIfig_pre[plotInd4CIfig_pre!=7] #dropping outlier from plot
getCIs(glmmSense,plotInd=plotInd4CIfig,col=c("darkblue"),main='',xlab='',xlim=c(0,100),cex.axis=1.4,lwd=2)
mtext('% Accuracy (Sensitivity)', side=3, line=.2)
abline(h=4.5,lty=1,col='darkgray')
abline(h=5.5,lty=1,col='darkgray')
getCIs(glmmSpec,plotInd=plotInd4CIfig,col=c("darkred"),axisLab=FALSE,main='',xlab='',xlim=c(0,100),lwd=2)
abline(h=4.5,lty=1,col='darkgray')
abline(h=5.5,lty=1,col='darkgray')
mtext('% Accuracy (Specificity)', side=3, line=.2)
########
```




Models for Learning
---------------


```{r learningModels, cache=TRUE}
#select users who did the survey at least twice, but exclude questions they saw twice.
users_with_multiple_tries<- unique(x$id[x$attemptNum>1])
multi_try_data_ind<-x$id %in% users_with_multiple_tries 
multi_try_data<-x[multi_try_data_ind& x$firstTry,]


#Calculate the percent of second attempts were discarded because users saw a duplicate plot
1-sum(multi_try_data_ind& x$firstTry)/sum(multi_try_data_ind)

cut=2 #cutoff point for max # of tries in our model (only look at first and second attempts)
multi_try_data_sense_leq_cut<- multi_try_data[multi_try_data$attemptNum<=cut & multi_try_data$trueSig,]
multi_try_data_spec_leq_cut <- multi_try_data[multi_try_data$attemptNum<=cut & !multi_try_data$trueSig,]

#Fit random intercept model
dim(multi_try_data_sense_leq_cut) #846 = # responses in model
glmmSenseLearn_rIntercept = glmer(correct~ 1 +  (1|id) + attemptNumFactor*style,
  data=multi_try_data_sense_leq_cut, family="binomial") # Fit model with interaction terms
print(glmmSenseLearn_rIntercept,correlation=FALSE)

dim(multi_try_data_spec_leq_cut) #859 = # responses in model
glmmSpecLearn_rIntercept = glmer(correct ~ 1 +  (1|id) + attemptNumFactor*style,
	data=multi_try_data_spec_leq_cut, family="binomial") #
print(glmmSpecLearn_rIntercept,correlation=FALSE) 




#Show variance explained by random intercepts
get_var_explained_by_rand_int(glmmSenseLearn_rIntercept)
get_var_explained_by_rand_int(glmmSpecLearn_rIntercept)

#Get CIs for learning models
# Get confidence intervals for the fitted accuracy rates for each combination of style and attempt number in the learning model. 
# Also get confidence intervals for odds ratios for the learning effect in each category.
# To get interpretation of effect of attemptNo, we need to add attemptNum coeff to interaction terms. 
#Negative interaction just means that the learning effect is less strong than in the reference category.
#The function below returns a list of CI matrixes for each style, with rows for attemptNum.
getCIlearn<-function(model,ci_width_scalar=1.96,test_type='two_sided'){    
	
	modelCoef<-fixef(model)
	coefNames<-names(modelCoef)

	#find value of 'cut' (max attemptNum in model)
	cut=length(coefNames)/K #K = number of categories for each attemptNum

	#list output by style
	CImats<-list()

	for(k in 1:K){
		k_mat<-matrix(NA,cut,8)
		colnames(k_mat)<-c('centerProb','liProb','uiProb','centerOR','liOR','uiOR','zstat','p_value') #we'll add this to CImats later
		#attemptNum is the row index of k_mat
		for(no in 1:cut){
      #a vector to multiply by the model coefficients
			a<-rep(0,times=length(coefNames))
			names(a)<-coefNames
			a['(Intercept)']<-1
			if(k>1) a[paste0('style',uStyle[k])] <-1
			if(no>1) a[paste0('attemptNumFactor',no)]<-1
			if(k>1 & no>1) a[paste0('attemptNumFactor',no,':style',uStyle[k])]	<-1

			#let a be the vector such that a'modelCoef = intercept + coefficient k
			#abbreviate this t(a)%*%modelCoef as af
			var_af<- t(a) %*% vcov(model) %*% a
			se_af<-sqrt(as.numeric(var_af))

			center_logOdds <- crossprod(a,modelCoef)
			k_mat[no,'uiProb']<- invLogit( center_logOdds + ci_width_scalar*se_af)
			k_mat[no,'liProb']<- invLogit( center_logOdds - ci_width_scalar*se_af)
			k_mat[no,'centerProb']<- invLogit(center_logOdds)
    

			#pvalues need to be calculated the same way, but without the intercept term, and without the baseline style term.
			#Get dist of "wf"=t(w)%*%coefficients, wwhere w is a vector similar to "a", above.
			if(no==1) next #This not relevant if a attemptNum=1
      
			w<-rep(0,times=length(coefNames))
      names(w)<-coefNames
      w[paste0('attemptNumFactor',no)]<-1
			if(k>1) w[paste0('attemptNumFactor',no,':style',uStyle[k])]	<-1

			var_wf<-t(w) %*% vcov(model) %*% w
			se_wf<-sqrt(as.numeric(var_wf))
      
      k_mat[no,'centerOR']<- exp(crossprod(w,modelCoef))
      k_mat[no,'uiOR']<- exp( crossprod(w,modelCoef) + ci_width_scalar*se_wf)
  		k_mat[no,'liOR']<- exp( crossprod(w,modelCoef) - ci_width_scalar*se_wf)
      
			zstat<- t(w)%*%modelCoef / se_wf
			if(test_type=="two_sided")      p_value<- 2*(1-pnorm(abs(zstat)))
			if(test_type=="one_sided_up")   p_value<-   (1-pnorm(zstat))
			if(test_type=="one_sided_down") p_value<-      pnorm(zstat)
			k_mat[no,'zstat']<- zstat
			k_mat[no,'p_value']<- p_value

		}
		CImats[[ uStyle[k] ]]<-k_mat
	}
	return(CImats)
}

#Show confidence intervals for odds ratios regarding the learning effect, for each category.
getCIlearn(glmmSenseLearn_rIntercept) 
getCIlearn(glmmSpecLearn_rIntercept)

```

Generate Figure 2

```{r figure2}


#Make figure 2

#Function for adding jittered background points to a plot.
plot_pts_for_CI_plot <- function(plotStyle='n100ref',type='sense',data=multi_try_data,cut=cut,jit_factor_x=1.2,jit_factor_y=1.2,y2lab,...){
  #style %in% plotStyle lets us use the same function for an aggregate plot
	ind1<- data$attemptNum <= cut
	ind2<- !data$trueSig
	if(type=='sense') ind2 <- data$trueSig
	ind3<-data$style %in% plotStyle
	d<- data[ind1 & ind2 & ind3,]
	plot(jitter(d$attemptNum,factor=jit_factor_x),jitter(d$correct+0,factor=jit_factor_y),col='darkgray',cex=.5,xaxt='n',yaxt='n', ...)
	axis(4,at=0:1,labels=y2lab,cex.axis=1.2)
}

#Function for adding CIs to a plot (CIs for accuracy)
add2plot_LearnCImats<-function(ciMat, ...){
	cut<-dim(ciMat)[1]
	plotCI(x=1:cut,y=ciMat[,'centerProb'],ui=ciMat[,'uiProb'],li=ciMat[,'liProb'],pch=19,cex=.5,add=TRUE, ...)
	axis(1, at=(1:cut), labels=1:cut) #need to reorder labels so they go down, not up
	yLabels<-seq(0,1,length=6)
	axis(2, at=yLabels, labels=yLabels) #need to reorder labels so they go down, not up
	abline(h=.5,lty=2,col='darkgray')	
}

#SENSE
ciMat_learn_sense<-getCIlearn(glmmSenseLearn_rIntercept)

par(mfrow=c(2,3),mar=c(4,3,2,1),oma=c(3,3,3,3))
plot_pts_for_CI_plot(plotStyle='n100ref',type='sense',cut=cut,jit_factor_x=1,jit_factor_y=1,y2lab=c('',''),xlab='',ylab='')
add2plot_LearnCImats(ciMat_learn_sense[['n100ref']],col='darkblue',lwd=2)
mtext(text='% Accuracy (Sensitivity)',side=2,line=2.5,cex=.85)
mtext(text='Reference',side=3,line=2.5,font=2)
mtext(text='Truly Significant',side=3,line=0.2,cex=.899)

plot_pts_for_CI_plot(plotStyle='n35',type='sense',cut=cut,jit_factor_x=1,jit_factor_y=1,y2lab=c('',''),ylab='',xlab='')
add2plot_LearnCImats(ciMat_learn_sense[['n35']],main='Lower n',col='darkblue',lwd=2)
mtext(text='Lower n',side=3,line=2.5,font=2)
mtext(text='Truly Significant',side=3,line=0.2,cex=.899)

plot_pts_for_CI_plot(plotStyle='bestFit',type='sense',cut=cut,jit_factor_x=1,jit_factor_y=1,xlab='',ylab='',y2lab=c('Guess Not Sig','Guess Sig'))
add2plot_LearnCImats(ciMat_learn_sense[['bestFit']],main='Best-fit Line',col='darkblue',ylab='',lwd=2,xlab='')
mtext(text='Best Fit',side=3,line=2.5,font=2)
mtext(text='Truly Significant',side=3,line=0.2,cex=.899)



#SPEC
ciMat_learn_spec<-getCIlearn(glmmSpecLearn_rIntercept)

plot_pts_for_CI_plot(plotStyle='n100ref',type='spec',xlab='',ylab='',cut=cut,jit_factor_x=1,jit_factor_y=1,y2lab=c('',''))
add2plot_LearnCImats(ciMat_learn_spec[['n100ref']],main='',col='darkred',ylab='Specificity',ylim=c(-.2,1.2),xlim=c(.5,cut+.5),lwd=2)
mtext(text='Not Significant',side=3,line=0.2,cex=.899)
mtext(text='% Accuracy (Specificity)',side=2,line=2.5,cex=.85)
mtext(text='Attempt Number',side=1,line=2.5,cex=.85)

plot_pts_for_CI_plot(plotStyle='n35',type='spec',xlab='',ylab='',cut=cut,jit_factor_x=1,jit_factor_y=1,y2lab=c('',''))
add2plot_LearnCImats(ciMat_learn_spec[['n35']],main='',col='darkred',ylab='',ylim=c(-.2,1.2),xlim=c(.5,cut+.5),lwd=2)
mtext(text='Not Significant ',side=3,line=0.2,cex=.899)
mtext(text='Attempt Number',side=1,line=2.5,cex=.85)

plot_pts_for_CI_plot(plotStyle='bestFit',type='spec',main='',xlab='',cut=cut,jit_factor_x=1,jit_factor_y=1,ylab='',y2lab=c('Guess Sig','Guess Not Sig'))
add2plot_LearnCImats(ciMat_learn_spec[['bestFit']],main='',col='darkred',lwd=2)
mtext(text='Not Significant ',side=3,line=0.2,cex=.899)
mtext(text='Attempt Number',side=1,line=2.5,cex=.85)




```




